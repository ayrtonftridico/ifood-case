{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163ae386-d071-4fc1-b4b6-70386cc84ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Diretórios\n",
    "# -----------------------------\n",
    "DIRS = {\n",
    "    \"yellow\": \"/Volumes/workspace/nyc_taxi/raw/yellow\",\n",
    "    \"green\":  \"/Volumes/workspace/nyc_taxi/raw/green\",\n",
    "    \"fhv\":    \"/Volumes/workspace/nyc_taxi/raw/fhv\",\n",
    "    \"fhvhv\":  \"/Volumes/workspace/nyc_taxi/raw/fhvhv\",\n",
    "}\n",
    "\n",
    "# --------------------------------------\n",
    "# Destino\n",
    "# --------------------------------------\n",
    "TABLES = {\n",
    "    \"yellow\": \"workspace.nyc_taxi.yellow_trips_bronze\",\n",
    "    \"green\":  \"workspace.nyc_taxi.green_trips_bronze\",\n",
    "    \"fhv\":    \"workspace.nyc_taxi.fhv_trips_bronze\",\n",
    "    \"fhvhv\":  \"workspace.nyc_taxi.fhvhv_trips_bronze\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Coluna de pickup utilizada para derivar 'anomes' (YYYYMM)\n",
    "# ---------------------------------------------------------\n",
    "PICKUP_COL = {\n",
    "    \"yellow\": \"tpep_pickup_datetime\",\n",
    "    \"green\":  \"lpep_pickup_datetime\",\n",
    "    \"fhv\":    \"pickup_datetime\",\n",
    "    \"fhvhv\":  \"pickup_datetime\",\n",
    "}\n",
    "\n",
    "# --------------------------------------\n",
    "# Schemas\n",
    "# --------------------------------------\n",
    "TARGET_YELLOW = {\n",
    "    \"vendorid\": \"long\",\n",
    "    \"tpep_pickup_datetime\": \"timestamp\",\n",
    "    \"tpep_dropoff_datetime\": \"timestamp\",\n",
    "    \"passenger_count\": \"integer\",\n",
    "    \"trip_distance\": \"double\",\n",
    "    \"ratecodeid\": \"long\",\n",
    "    \"store_and_fwd_flag\": \"string\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"payment_type\": \"long\",\n",
    "    \"fare_amount\": \"double\",\n",
    "    \"extra\": \"double\",\n",
    "    \"mta_tax\": \"double\",\n",
    "    \"tip_amount\": \"double\",\n",
    "    \"tolls_amount\": \"double\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"total_amount\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"airport_fee\": \"double\",\n",
    "}\n",
    "\n",
    "TARGET_GREEN = {\n",
    "    \"vendorid\": \"long\",\n",
    "    \"lpep_pickup_datetime\": \"timestamp\",\n",
    "    \"lpep_dropoff_datetime\": \"timestamp\",\n",
    "    \"passenger_count\": \"integer\",\n",
    "    \"trip_distance\": \"double\",\n",
    "    \"ratecodeid\": \"long\",\n",
    "    \"store_and_fwd_flag\": \"string\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"payment_type\": \"long\",\n",
    "    \"fare_amount\": \"double\",\n",
    "    \"extra\": \"double\",\n",
    "    \"mta_tax\": \"double\",\n",
    "    \"tip_amount\": \"double\",\n",
    "    \"tolls_amount\": \"double\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"total_amount\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"airport_fee\": \"double\",\n",
    "}\n",
    "\n",
    "TARGET_FHV = {\n",
    "    \"dispatching_base_num\": \"string\",\n",
    "    \"pickup_datetime\": \"timestamp\",\n",
    "    \"dropoff_datetime\": \"timestamp\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"sr_flag\": \"integer\",\n",
    "    \"affiliated_base_number\": \"string\",\n",
    "}\n",
    "\n",
    "TARGET_FHVHV = {\n",
    "    \"hvfhs_license_num\": \"string\",\n",
    "    \"dispatching_base_num\": \"string\",\n",
    "    \"pickup_datetime\": \"timestamp\",\n",
    "    \"dropoff_datetime\": \"timestamp\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"originating_base_num\": \"string\",\n",
    "    \"sr_flag\": \"integer\",\n",
    "}\n",
    "\n",
    "# -----------------------------------\n",
    "# Funções\n",
    "# -----------------------------------\n",
    "def list_parquets(base_dir: str):\n",
    "    \"\"\"\n",
    "    Lista todos os arquivos .parquet no diretório.\n",
    "    \"\"\"\n",
    "    return [f.path for f in dbutils.fs.ls(base_dir) if f.path.endswith(\".parquet\")]\n",
    "\n",
    "def _sanitize(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza o nome de uma coluna:\n",
    "      - trim + lower()\n",
    "      - espaços/traços → underscore\n",
    "      - múltiplos underscores → underscore único\n",
    "    Garante consistência entre meses/anos com capitalizações diferentes.\n",
    "    \"\"\"\n",
    "    n = name.strip().lower()\n",
    "    n = re.sub(r\"[ \\t\\-]+\", \"_\", n)\n",
    "    n = re.sub(r\"__+\", \"_\", n)\n",
    "    return n\n",
    "\n",
    "def to_lower_columns(df):\n",
    "    \"\"\"\n",
    "    Renomeia TODAS as colunas do DataFrame para nomes normalizados (lower + underscore).\n",
    "    Resolve potenciais colisões geradas pela normalização adicionando sufixo __dupN.\n",
    "    Ex.: 'DropOff_datetime' e 'dropoff_datetime' → 'dropoff_datetime' / 'dropoff_datetime__dup1'\n",
    "    \"\"\"\n",
    "    current = df.columns\n",
    "    used = set()\n",
    "    for c in current:\n",
    "        new = _sanitize(c)\n",
    "        if new in used:\n",
    "            # Evita colisões criando um sufixo incremental\n",
    "            i, cand = 1, f\"{new}__dup1\"\n",
    "            while cand in used:\n",
    "                i += 1\n",
    "                cand = f\"{new}__dup{i}\"\n",
    "            new = cand\n",
    "        if new != c:\n",
    "            df = df.withColumnRenamed(c, new)\n",
    "        used.add(new)\n",
    "    return df\n",
    "\n",
    "def ensure_schema(df, target_schema: dict):\n",
    "    \"\"\"\n",
    "    Garante o schema alvo:\n",
    "      - cria colunas ausentes como NULL (cast para o tipo alvo)\n",
    "      - faz cast das colunas existentes para o tipo desejado\n",
    "    Isso neutraliza divergências entre anos (ex.: DOUBLE vs INT64).\n",
    "    \"\"\"\n",
    "    for col, dtype in target_schema.items():\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None).cast(dtype))\n",
    "        else:\n",
    "            df = df.withColumn(col, F.col(col).cast(dtype))\n",
    "    return df\n",
    "\n",
    "def derive_anomes_from(df, pickup_col_name: str):\n",
    "    \"\"\"\n",
    "    Cria 'anomes' (YYYYMM) a partir da **coluna de pickup informada**.\n",
    "    \"\"\"\n",
    "    col_norm = _sanitize(pickup_col_name)            # normaliza o nome esperado\n",
    "    if col_norm not in df.columns:\n",
    "        # Dica: logue df.columns para inspecionar o arquivo problemático\n",
    "        raise ValueError(\n",
    "            f\"Coluna de pickup '{pickup_col_name}' (normalizada: '{col_norm}') não encontrada. \"\n",
    "            f\"Colunas disponíveis: {df.columns}\"\n",
    "        )\n",
    "    return df.withColumn(\"anomes\", F.date_format(F.col(col_norm), \"yyyyMM\"))\n",
    "\n",
    "def normalize_one_file(path: str, target_schema: dict, pickup_col_name: str):\n",
    "    \"\"\"\n",
    "      1) Leitura\n",
    "      2) Normalização de nomes\n",
    "      3) Aplicação do schema alvo (casts + colunas ausentes)\n",
    "      4) Derivação da partição 'anomes' (YYYYMM) a partir da coluna de pickup informada\n",
    "      5) Seleção apenas do schema alvo + 'anomes'\n",
    "    \"\"\"\n",
    "    # 1) Leitura\n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    # 2) Nomes normalizados\n",
    "    df = to_lower_columns(df)\n",
    "\n",
    "    # 3) Tipagem/colunas segundo o schema alvo\n",
    "    df = ensure_schema(df, target_schema)\n",
    "\n",
    "    # 4) Partição derivada a partir do pickup\n",
    "    df = derive_anomes_from(df, pickup_col_name)\n",
    "\n",
    "    # 5) Seleciona schema + partição\n",
    "    select_cols = list(target_schema.keys()) + [\"anomes\"]\n",
    "    return df.select(*select_cols).filter(F.col(\"anomes\").isNotNull())\n",
    "\n",
    "def build_bronze_for_category(category: str, base_dir: str, target_schema: dict,\n",
    "                              pickup_col_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Constrói a tabela Bronze de UMA categoria:\n",
    "      - Lista e normaliza todos os arquivos do diretório\n",
    "      - Une com unionByName (tolerante à ordem/colunas)\n",
    "      - Loga a volumetria por partição (anomes -> linhas)\n",
    "      - Sobrescreve a tabela Delta particionada por 'anomes'\n",
    "    Retorna o DataFrame final consolidado.\n",
    "    \"\"\"\n",
    "    # Lista arquivos Parquet da categoria\n",
    "    files = list_parquets(base_dir)\n",
    "    assert files, f\"Nenhum parquet encontrado em {base_dir}\"\n",
    "    print(f\"🔎 {category}: {len(files)} arquivo(s) em {base_dir}\")\n",
    "\n",
    "    # Normaliza todos os arquivos individualmente\n",
    "    dfs, skipped = [], 0\n",
    "    for p in files:\n",
    "        try:\n",
    "            df_one = normalize_one_file(\n",
    "                path=p,\n",
    "                target_schema=target_schema,\n",
    "                pickup_col_name=pickup_col_name,\n",
    "            )\n",
    "            dfs.append(df_one)\n",
    "        except ValueError as e:\n",
    "            print(f\"⚠️ Ignorando arquivo sem coluna '{pickup_col_name}': {p} | {e}\")\n",
    "            skipped += 1\n",
    "\n",
    "    # União segura de todos os pedaços normalizados\n",
    "    df_all = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Log por partição\n",
    "    # ---------------------------\n",
    "\n",
    "    counts_df = df_all.groupBy(\"anomes\").count().orderBy(\"anomes\")\n",
    "    print(f\"📆 Partições detectadas para {category} (anomes -> linhas):\")\n",
    "    for row in counts_df.collect():\n",
    "        print(f\"  - {row['anomes']}: {row['count']:,}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Escrita\n",
    "    # ---------------------------\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    (df_all.repartition(\"anomes\")          # melhora distribuição por partição na escrita\n",
    "          .write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .partitionBy(\"anomes\")\n",
    "          .saveAsTable(table_name))\n",
    "\n",
    "    total = df_all.count()\n",
    "    print(f\"✅ Bronze '{category}' criada: {table_name} | Linhas: {total:,}\")\n",
    "    return df_all\n",
    "\n",
    "# -------------------------\n",
    "#         Execução \n",
    "# -------------------------\n",
    "\n",
    "df_yellow = build_bronze_for_category(\n",
    "    category=\"yellow\",\n",
    "    base_dir=DIRS[\"yellow\"],\n",
    "    target_schema=TARGET_YELLOW,\n",
    "    pickup_col_name=PICKUP_COL[\"yellow\"],\n",
    "    table_name=TABLES[\"yellow\"]\n",
    ")\n",
    "\n",
    "df_green = build_bronze_for_category(\n",
    "    category=\"green\",\n",
    "    base_dir=DIRS[\"green\"],\n",
    "    target_schema=TARGET_GREEN,\n",
    "    pickup_col_name=PICKUP_COL[\"green\"],\n",
    "    table_name=TABLES[\"green\"]\n",
    ")\n",
    "\n",
    "df_fhv = build_bronze_for_category(\n",
    "    category=\"fhv\",\n",
    "    base_dir=DIRS[\"fhv\"],\n",
    "    target_schema=TARGET_FHV,\n",
    "    pickup_col_name=PICKUP_COL[\"fhv\"],\n",
    "    table_name=TABLES[\"fhv\"]\n",
    ")\n",
    "\n",
    "df_fhvhv = build_bronze_for_category(\n",
    "    category=\"fhvhv\",\n",
    "    base_dir=DIRS[\"fhvhv\"],\n",
    "    target_schema=TARGET_FHVHV,\n",
    "    pickup_col_name=PICKUP_COL[\"fhvhv\"],\n",
    "    table_name=TABLES[\"fhvhv\"]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Amostras\n",
    "# -------------------------\n",
    "print(\"🔎 Amostras das tabelas Bronze:\")\n",
    "display(spark.table(TABLES[\"yellow\"]).limit(5))\n",
    "display(spark.table(TABLES[\"green\"]).limit(5))\n",
    "display(spark.table(TABLES[\"fhv\"]).limit(5))\n",
    "display(spark.table(TABLES[\"fhvhv\"]).limit(5))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5860577388461928,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestao_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
