{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163ae386-d071-4fc1-b4b6-70386cc84ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# BRONZE (SOR) ‚Äì NYC TLC ‚Äì Leitura & Padroniza√ß√£o (Delta Lake)\n",
    "# =============================================================\n",
    "# Objetivo:\n",
    "# - Ler arquivos Parquet de m√∫ltiplas frotas (yellow, green, fhv, fhvhv)\n",
    "# - Normalizar nomes de colunas (lowercase + underscores)\n",
    "# - Garantir um schema-alvo por categoria (casts + colunas ausentes como NULL)\n",
    "# - Derivar coluna de parti√ß√£o 'anomes' (YYYYMM) a partir de UMA coluna expl√≠cita de pickup\n",
    "# - Persistir em tabelas Delta particionadas por 'anomes'\n",
    "# - Logar a volumetria por parti√ß√£o (uma linha por parti√ß√£o) para facilitar auditoria\n",
    "#\n",
    "# Observa√ß√µes:\n",
    "# - O c√≥digo ignora arquivos que n√£o possuam a coluna de pickup esperada (por categoria).\n",
    "# - Evitamos leituras recursivas e unificamos usando unionByName (tolerante √† ordem/colunas).\n",
    "# - Caso deseje incluir tamb√©m Yellow/Green, basta descomentar as chamadas ao final.\n",
    "# =============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Diret√≥rios-fonte (Volumes UC)\n",
    "# -----------------------------\n",
    "# Ajuste os paths conforme o seu ambiente/volume no Databricks Unity Catalog.\n",
    "DIRS = {\n",
    "    \"yellow\": \"/Volumes/workspace/nyc_taxi/raw/yellow\",\n",
    "    \"green\":  \"/Volumes/workspace/nyc_taxi/raw/green\",\n",
    "    \"fhv\":    \"/Volumes/workspace/nyc_taxi/raw/fhv\",\n",
    "    \"fhvhv\":  \"/Volumes/workspace/nyc_taxi/raw/fhvhv\",\n",
    "}\n",
    "\n",
    "# --------------------------------------\n",
    "# Destino: nomes das Tabelas Delta Bronze\n",
    "# --------------------------------------\n",
    "# Cada categoria grava em uma tabela distinta, particionada por 'anomes'.\n",
    "TABLES = {\n",
    "    \"yellow\": \"workspace.nyc_taxi.yellow_trips_bronze\",\n",
    "    \"green\":  \"workspace.nyc_taxi.green_trips_bronze\",\n",
    "    \"fhv\":    \"workspace.nyc_taxi.fhv_trips_bronze\",\n",
    "    \"fhvhv\":  \"workspace.nyc_taxi.fhvhv_trips_bronze\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Coluna de pickup utilizada para derivar 'anomes' (YYYYMM)\n",
    "# ---------------------------------------------------------\n",
    "# A deriva√ß√£o de parti√ß√µes sempre usar√° explicitamente a coluna definida aqui,\n",
    "# evitando heur√≠sticas/‚Äúadivinha√ß√£o‚Äù de nomes.\n",
    "PICKUP_COL = {\n",
    "    \"yellow\": \"tpep_pickup_datetime\",\n",
    "    \"green\":  \"lpep_pickup_datetime\",\n",
    "    \"fhv\":    \"pickup_datetime\",\n",
    "    \"fhvhv\":  \"pickup_datetime\",\n",
    "}\n",
    "\n",
    "# --------------------------------------\n",
    "# Schemas-alvo por categoria (padroniza√ß√£o)\n",
    "# --------------------------------------\n",
    "# Dica: se for validar tipos com amostras, confira se valores monet√°rios n√£o\n",
    "# est√£o em inteiro/float misturados e se IDs est√£o coerentes (inteiros).\n",
    "# Aqui optamos por tipos simples (long/double/integer/string/timestamp)\n",
    "# para robustez cross-anos; na Silver voc√™ pode refinar (ex.: DECIMAL).\n",
    "TARGET_YELLOW = {\n",
    "    \"vendorid\": \"long\",\n",
    "    \"tpep_pickup_datetime\": \"timestamp\",\n",
    "    \"tpep_dropoff_datetime\": \"timestamp\",\n",
    "    \"passenger_count\": \"integer\",\n",
    "    \"trip_distance\": \"double\",\n",
    "    \"ratecodeid\": \"long\",\n",
    "    \"store_and_fwd_flag\": \"string\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"payment_type\": \"long\",\n",
    "    \"fare_amount\": \"double\",\n",
    "    \"extra\": \"double\",\n",
    "    \"mta_tax\": \"double\",\n",
    "    \"tip_amount\": \"double\",\n",
    "    \"tolls_amount\": \"double\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"total_amount\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"airport_fee\": \"double\",\n",
    "}\n",
    "\n",
    "TARGET_GREEN = {\n",
    "    \"vendorid\": \"long\",\n",
    "    \"lpep_pickup_datetime\": \"timestamp\",\n",
    "    \"lpep_dropoff_datetime\": \"timestamp\",\n",
    "    \"passenger_count\": \"integer\",\n",
    "    \"trip_distance\": \"double\",\n",
    "    \"ratecodeid\": \"long\",\n",
    "    \"store_and_fwd_flag\": \"string\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"payment_type\": \"long\",\n",
    "    \"fare_amount\": \"double\",\n",
    "    \"extra\": \"double\",\n",
    "    \"mta_tax\": \"double\",\n",
    "    \"tip_amount\": \"double\",\n",
    "    \"tolls_amount\": \"double\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"total_amount\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"airport_fee\": \"double\",\n",
    "}\n",
    "\n",
    "TARGET_FHV = {\n",
    "    \"dispatching_base_num\": \"string\",\n",
    "    \"pickup_datetime\": \"timestamp\",\n",
    "    \"dropoff_datetime\": \"timestamp\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"sr_flag\": \"integer\",\n",
    "    \"affiliated_base_number\": \"string\",\n",
    "}\n",
    "\n",
    "TARGET_FHVHV = {\n",
    "    \"hvfhs_license_num\": \"string\",\n",
    "    \"dispatching_base_num\": \"string\",\n",
    "    \"pickup_datetime\": \"timestamp\",\n",
    "    \"dropoff_datetime\": \"timestamp\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"originating_base_num\": \"string\",\n",
    "    \"sr_flag\": \"integer\",\n",
    "}\n",
    "\n",
    "# -----------------------------------\n",
    "# Utilit√°rios: listagem & normaliza√ß√£o\n",
    "# -----------------------------------\n",
    "def list_parquets(base_dir: str):\n",
    "    \"\"\"\n",
    "    Lista todos os arquivos .parquet no diret√≥rio (n√£o recursivo).\n",
    "    Observa√ß√£o: caso use subpastas por ano/m√™s no futuro, adapte para varrer recursivamente.\n",
    "    \"\"\"\n",
    "    return [f.path for f in dbutils.fs.ls(base_dir) if f.path.endswith(\".parquet\")]\n",
    "\n",
    "def _sanitize(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza o nome de uma coluna:\n",
    "      - trim + lower()\n",
    "      - espa√ßos/tra√ßos ‚Üí underscore\n",
    "      - m√∫ltiplos underscores ‚Üí underscore √∫nico\n",
    "    Garante consist√™ncia entre meses/anos com capitaliza√ß√µes diferentes.\n",
    "    \"\"\"\n",
    "    n = name.strip().lower()\n",
    "    n = re.sub(r\"[ \\t\\-]+\", \"_\", n)\n",
    "    n = re.sub(r\"__+\", \"_\", n)\n",
    "    return n\n",
    "\n",
    "def to_lower_columns(df):\n",
    "    \"\"\"\n",
    "    Renomeia TODAS as colunas do DataFrame para nomes normalizados (lower + underscore).\n",
    "    Resolve potenciais colis√µes geradas pela normaliza√ß√£o adicionando sufixo __dupN.\n",
    "    Ex.: 'DropOff_datetime' e 'dropoff_datetime' ‚Üí 'dropoff_datetime' / 'dropoff_datetime__dup1'\n",
    "    \"\"\"\n",
    "    current = df.columns\n",
    "    used = set()\n",
    "    for c in current:\n",
    "        new = _sanitize(c)\n",
    "        if new in used:\n",
    "            # Evita colis√µes criando um sufixo incremental\n",
    "            i, cand = 1, f\"{new}__dup1\"\n",
    "            while cand in used:\n",
    "                i += 1\n",
    "                cand = f\"{new}__dup{i}\"\n",
    "            new = cand\n",
    "        if new != c:\n",
    "            df = df.withColumnRenamed(c, new)\n",
    "        used.add(new)\n",
    "    return df\n",
    "\n",
    "def ensure_schema(df, target_schema: dict):\n",
    "    \"\"\"\n",
    "    Garante o schema alvo:\n",
    "      - cria colunas ausentes como NULL (cast para o tipo alvo)\n",
    "      - faz cast das colunas existentes para o tipo desejado\n",
    "    Isso neutraliza diverg√™ncias entre anos (ex.: DOUBLE vs INT64).\n",
    "    \"\"\"\n",
    "    for col, dtype in target_schema.items():\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None).cast(dtype))\n",
    "        else:\n",
    "            df = df.withColumn(col, F.col(col).cast(dtype))\n",
    "    return df\n",
    "\n",
    "def derive_anomes_from(df, pickup_col_name: str):\n",
    "    \"\"\"\n",
    "    Cria 'anomes' (YYYYMM) a partir da **coluna de pickup informada**.\n",
    "    Lan√ßa erro se a coluna n√£o existir AP√ìS normaliza√ß√£o de nomes,\n",
    "    for√ßando ajuste expl√≠cito (evita parti√ß√µes erradas/NULL silenciosas).\n",
    "    \"\"\"\n",
    "    col_norm = _sanitize(pickup_col_name)            # normaliza o nome esperado\n",
    "    if col_norm not in df.columns:\n",
    "        # Dica: logue df.columns para inspecionar o arquivo problem√°tico\n",
    "        raise ValueError(\n",
    "            f\"Coluna de pickup '{pickup_col_name}' (normalizada: '{col_norm}') n√£o encontrada. \"\n",
    "            f\"Colunas dispon√≠veis: {df.columns}\"\n",
    "        )\n",
    "    return df.withColumn(\"anomes\", F.date_format(F.col(col_norm), \"yyyyMM\"))\n",
    "\n",
    "def normalize_one_file(path: str, target_schema: dict, pickup_col_name: str):\n",
    "    \"\"\"\n",
    "    Pipeline de normaliza√ß√£o para UM arquivo parquet:\n",
    "      1) Leitura\n",
    "      2) Normaliza√ß√£o de nomes\n",
    "      3) Aplica√ß√£o do schema alvo (casts + colunas ausentes)\n",
    "      4) Deriva√ß√£o da parti√ß√£o 'anomes' (YYYYMM) a partir da coluna de pickup informada\n",
    "      5) Sele√ß√£o apenas do schema alvo + 'anomes'\n",
    "    Retorna um DataFrame pronto para uni√£o.\n",
    "    \"\"\"\n",
    "    # 1) Leitura\n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    # 2) Nomes normalizados\n",
    "    df = to_lower_columns(df)\n",
    "\n",
    "    # 3) Tipagem/colunas segundo o schema alvo\n",
    "    df = ensure_schema(df, target_schema)\n",
    "\n",
    "    # 4) Parti√ß√£o derivada a partir do pickup expl√≠cito\n",
    "    df = derive_anomes_from(df, pickup_col_name)\n",
    "\n",
    "    # 5) Seleciona apenas colunas relevantes + parti√ß√£o\n",
    "    select_cols = list(target_schema.keys()) + [\"anomes\"]\n",
    "    return df.select(*select_cols).filter(F.col(\"anomes\").isNotNull())\n",
    "\n",
    "def build_bronze_for_category(category: str, base_dir: str, target_schema: dict,\n",
    "                              pickup_col_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Constr√≥i a tabela Bronze de UMA categoria:\n",
    "      - Lista e normaliza todos os arquivos do diret√≥rio\n",
    "      - Une com unionByName (tolerante √† ordem/colunas)\n",
    "      - Loga a volumetria por parti√ß√£o (anomes -> linhas)\n",
    "      - Sobrescreve a tabela Delta particionada por 'anomes'\n",
    "    Retorna o DataFrame final consolidado.\n",
    "    \"\"\"\n",
    "    # Lista arquivos Parquet da categoria\n",
    "    files = list_parquets(base_dir)\n",
    "    assert files, f\"Nenhum parquet encontrado em {base_dir}\"\n",
    "    print(f\"üîé {category}: {len(files)} arquivo(s) em {base_dir}\")\n",
    "\n",
    "    # Normaliza todos os arquivos individualmente\n",
    "    dfs, skipped = [], 0\n",
    "    for p in files:\n",
    "        try:\n",
    "            df_one = normalize_one_file(\n",
    "                path=p,\n",
    "                target_schema=target_schema,\n",
    "                pickup_col_name=pickup_col_name,\n",
    "            )\n",
    "            dfs.append(df_one)\n",
    "        except ValueError as e:\n",
    "            # Arquivo sem a coluna de pickup esperada ‚Üí ignoramos para n√£o travar o job todo\n",
    "            print(f\"‚ö†Ô∏è Ignorando arquivo sem coluna '{pickup_col_name}': {p} | {e}\")\n",
    "            skipped += 1\n",
    "\n",
    "    # Se nenhum arquivo foi aproveit√°vel, aborta com mensagem clara\n",
    "    assert dfs, f\"Nenhum arquivo utiliz√°vel em {base_dir} (todos sem coluna '{pickup_col_name}'?).\"\n",
    "    if skipped:\n",
    "        print(f\"‚ÑπÔ∏è {category}: {skipped} arquivo(s) ignorado(s) por aus√™ncia de '{pickup_col_name}'.\")\n",
    "\n",
    "    # Uni√£o segura de todos os peda√ßos normalizados\n",
    "    df_all = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Log por parti√ß√£o (leg√≠vel)\n",
    "    # ---------------------------\n",
    "    # Em vez de display, imprimimos linha a linha para facilitar em logs/Jobs:\n",
    "    #   anomes: contagem_com_separador_de_milhar\n",
    "    counts_df = df_all.groupBy(\"anomes\").count().orderBy(\"anomes\")\n",
    "    print(f\"üìÜ Parti√ß√µes detectadas para {category} (anomes -> linhas):\")\n",
    "    for row in counts_df.collect():\n",
    "        print(f\"  - {row['anomes']}: {row['count']:,}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Escrita Delta particionada\n",
    "    # ---------------------------\n",
    "    # Sobrescreve a tabela (DROP impl√≠cito pelo modo overwrite).\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    (df_all.repartition(\"anomes\")          # melhora distribui√ß√£o por parti√ß√£o na escrita\n",
    "          .write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .partitionBy(\"anomes\")\n",
    "          .saveAsTable(table_name))\n",
    "\n",
    "    total = df_all.count()\n",
    "    print(f\"‚úÖ Bronze '{category}' criada: {table_name} | Linhas: {total:,}\")\n",
    "    return df_all\n",
    "\n",
    "# -------------------------\n",
    "# Execu√ß√£o (exemplos atuais)\n",
    "# -------------------------\n",
    "# Obs.: Se quiser rodar tamb√©m Yellow/Green, descomente as duas chamadas abaixo.\n",
    "\n",
    "# df_yellow = build_bronze_for_category(\n",
    "#     category=\"yellow\",\n",
    "#     base_dir=DIRS[\"yellow\"],\n",
    "#     target_schema=TARGET_YELLOW,\n",
    "#     pickup_col_name=PICKUP_COL[\"yellow\"],\n",
    "#     table_name=TABLES[\"yellow\"]\n",
    "# )\n",
    "\n",
    "# df_green = build_bronze_for_category(\n",
    "#     category=\"green\",\n",
    "#     base_dir=DIRS[\"green\"],\n",
    "#     target_schema=TARGET_GREEN,\n",
    "#     pickup_col_name=PICKUP_COL[\"green\"],\n",
    "#     table_name=TABLES[\"green\"]\n",
    "# )\n",
    "\n",
    "# Executando apenas FHV e FHVHV (como no seu fluxo atual)\n",
    "df_fhv = build_bronze_for_category(\n",
    "    category=\"fhv\",\n",
    "    base_dir=DIRS[\"fhv\"],\n",
    "    target_schema=TARGET_FHV,\n",
    "    pickup_col_name=PICKUP_COL[\"fhv\"],\n",
    "    table_name=TABLES[\"fhv\"]\n",
    ")\n",
    "\n",
    "df_fhvhv = build_bronze_for_category(\n",
    "    category=\"fhvhv\",\n",
    "    base_dir=DIRS[\"fhvhv\"],\n",
    "    target_schema=TARGET_FHVHV,\n",
    "    pickup_col_name=PICKUP_COL[\"fhvhv\"],\n",
    "    table_name=TABLES[\"fhvhv\"]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Amostras (opcional)\n",
    "# -------------------------\n",
    "# √ötil para validar o resultado de escrita rapidamente no notebook.\n",
    "print(\"üîé Amostras das tabelas Bronze:\")\n",
    "display(spark.table(TABLES[\"yellow\"]).limit(5))\n",
    "display(spark.table(TABLES[\"green\"]).limit(5))\n",
    "display(spark.table(TABLES[\"fhv\"]).limit(5))\n",
    "display(spark.table(TABLES[\"fhvhv\"]).limit(5))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5860577388461928,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestao_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
