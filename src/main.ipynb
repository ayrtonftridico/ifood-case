{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309c98f4-1072-4b9c-9f9e-9a2930c63b7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalize NYC Yellow Taxi Trips Data to Delta Lake"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# DiretÃ³rio base contendo todos os arquivos Parquet consolidados (2020â€“2025)\n",
    "base_dir = \"/Volumes/workspace/nyc_taxi/raw/full\"\n",
    "\n",
    "# Schema alvo\n",
    "target_schema = {\n",
    "    \"vendorid\": \"long\",                 # ID do fornecedor/tÃ¡xi (numÃ©rico inteiro)\n",
    "    \"tpep_pickup_datetime\": \"timestamp\",# Data/hora do embarque\n",
    "    \"tpep_dropoff_datetime\": \"timestamp\",# Data/hora do desembarque\n",
    "    \"passenger_count\": \"integer\",       # Quantidade de passageiros (inteiro)\n",
    "    \"trip_distance\": \"double\",          # DistÃ¢ncia da viagem (milhas)\n",
    "    \"ratecodeid\": \"long\",               # CÃ³digo da tarifa aplicada\n",
    "    \"store_and_fwd_flag\": \"string\",     # Flag de armazenamento/transmissÃ£o do dado (Y/N)\n",
    "    \"pulocationid\": \"long\",             # ID da localizaÃ§Ã£o de embarque\n",
    "    \"dolocationid\": \"long\",             # ID da localizaÃ§Ã£o de desembarque\n",
    "    \"payment_type\": \"long\",             # Tipo de pagamento (1=Dinheiro, 2=CartÃ£o, etc.)\n",
    "    \"fare_amount\": \"double\",            # Valor da corrida sem taxas adicionais\n",
    "    \"extra\": \"double\",                  # Valor de extras (ex.: taxa noturna)\n",
    "    \"mta_tax\": \"double\",                # Taxa da MTA\n",
    "    \"tip_amount\": \"double\",             # Valor de gorjeta\n",
    "    \"tolls_amount\": \"double\",           # Valor de pedÃ¡gios\n",
    "    \"improvement_surcharge\": \"double\",  # Taxa de melhoria\n",
    "    \"total_amount\": \"double\",           # Valor total da corrida\n",
    "    \"congestion_surcharge\": \"double\",   # Taxa de congestionamento\n",
    "    \"airport_fee\": \"double\",            # Taxa de aeroporto (quando aplicÃ¡vel)\n",
    "}\n",
    "\n",
    "target_cols = list(target_schema.keys())\n",
    "\n",
    "# ðŸ“‚ Lista todos os arquivos Parquet encontrados no diretÃ³rio base\n",
    "files = [f.path for f in dbutils.fs.ls(base_dir) if f.path.endswith(\".parquet\")]\n",
    "assert files, f\"Nenhum parquet encontrado em {base_dir}\"\n",
    "print(f\"ðŸ”Ž Encontrados {len(files)} arquivos\")\n",
    "\n",
    "def normalize_one(path: str):\n",
    "    \"\"\"\n",
    "    FunÃ§Ã£o responsÃ¡vel por:\n",
    "    - Ler o arquivo Parquet individualmente.\n",
    "    - Padronizar nomes de colunas para minÃºsculas.\n",
    "    - Garantir que todas as colunas do schema alvo existam (mesmo que com NULL).\n",
    "    - Realizar cast de cada coluna para o tipo correto.\n",
    "    - Criar a coluna 'anomes' no formato YYYYMM a partir da data de pickup.\n",
    "    - Retornar apenas colunas do schema alvo + partiÃ§Ã£o.\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    # Normaliza nomes de colunas para lowercase\n",
    "    for c in df.columns:\n",
    "        df = df.withColumnRenamed(c, c.lower())\n",
    "\n",
    "    # Adiciona colunas ausentes e aplica cast para o tipo definido no schema alvo\n",
    "    for col, dtype in target_schema.items():\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None).cast(dtype))\n",
    "        else:\n",
    "            df = df.withColumn(col, F.col(col).cast(dtype))\n",
    "\n",
    "    # Cria coluna de partiÃ§Ã£o com base na data de pickup\n",
    "    df = df.withColumn(\"anomes\", F.date_format(F.col(\"tpep_pickup_datetime\"), \"yyyyMM\"))\n",
    "\n",
    "    # Seleciona somente colunas relevantes e descarta registros sem data vÃ¡lida\n",
    "    return df.select(*target_cols, \"anomes\").filter(F.col(\"anomes\").isNotNull())\n",
    "\n",
    "# ðŸ”„ Normaliza todos os arquivos individualmente\n",
    "dfs = [normalize_one(p) for p in files]\n",
    "\n",
    "# ðŸ”— Une todos os DataFrames normalizados de forma segura\n",
    "df_all = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "# ðŸš® Remove partiÃ§Ãµes \"outliers\" com baixa contagem (menos de 10k linhas)\n",
    "part_counts = df_all.groupBy(\"anomes\").count().filter(F.col(\"count\") >= 10000)\n",
    "valid_partitions = [row[\"anomes\"] for row in part_counts.collect()]\n",
    "df_all = df_all.filter(F.col(\"anomes\").isin(valid_partitions))\n",
    "\n",
    "# ðŸ“Š Valida partiÃ§Ãµes resultantes\n",
    "print(\"ðŸ“† PartiÃ§Ãµes encontradas:\")\n",
    "display(df_all.select(\"anomes\").distinct().orderBy(\"anomes\"))\n",
    "\n",
    "# ðŸš® Dropa a tabela anterior existente, para caso de mudanÃ§a de schema\n",
    "spark.sql(\"DROP TABLE IF EXISTS workspace.nyc_taxi.yellowtaxi_trips_sor\")\n",
    "\n",
    "# ðŸ’¾ Escreve o resultado em uma tabela Delta Lake particionada por 'anomes'\n",
    "(df_all.repartition(\"anomes\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .partitionBy(\"anomes\")\n",
    "      .saveAsTable(\"workspace.nyc_taxi.yellowtaxi_trips_sor\"))\n",
    "\n",
    "print(f\"âœ… Base SOR criada com sucesso! Total de linhas: {df_all.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0706e77f-51bb-4d41-858b-264882d11e0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NYC Yellow Taxi Trips Data Processing"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ðŸ“¥ Leitura da tabela Bronze (camada SOR criada anteriormente)\n",
    "df_bronze = spark.table(\"workspace.nyc_taxi.yellowtaxi_trips_sor\")\n",
    "\n",
    "# ðŸŽ¯ SeleÃ§Ã£o e filtragem para criar a camada Silver\n",
    "df_silver = (\n",
    "    df_bronze\n",
    "    .select(\n",
    "        \"vendorid\",              # ID do fornecedor/tÃ¡xi\n",
    "        \"passenger_count\",       # NÃºmero de passageiros\n",
    "        \"total_amount\",          # Valor total da corrida\n",
    "        \"tpep_pickup_datetime\",  # Data/hora de embarque\n",
    "        \"tpep_dropoff_datetime\", # Data/hora de desembarque\n",
    "        \"anomes\"                 # Coluna de partiÃ§Ã£o no formato YYYYMM\n",
    "    )\n",
    "    .filter(\n",
    "        (F.col(\"anomes\") >= \"202301\") &  # Inclui apenas dados de janeiro/2023\n",
    "        (F.col(\"anomes\") <= \"202305\")    # atÃ© maio/2023 (inclusive)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ðŸ’¾ Escrita da camada Silver\n",
    "(df_silver.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")           # Sobrescreve a tabela se ela jÃ¡ existir\n",
    "    .partitionBy(\"anomes\")       # Cria partiÃ§Ãµes fÃ­sicas por mÃªs\n",
    "    .saveAsTable(\"workspace.nyc_taxi.yellowtaxi_trips_2023_spec\"))\n",
    "\n",
    "# ðŸ‘€ VisualizaÃ§Ã£o de amostra para conferÃªncia\n",
    "display(df_silver.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d553114b-a338-468a-a522-6a6e9b6fea39",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NYC Yellow Taxi Trips Count by Month"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT anomes,\n",
    "       COUNT(*) AS qtd_linhas\n",
    "FROM workspace.nyc_taxi.yellowtaxi_trips_2023_spec\n",
    "GROUP BY anomes\n",
    "ORDER BY anomes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "044be0a9-fa98-4f1a-8b2b-661f8ecb072f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try catch\n",
    "delta\n",
    "volume de arquivos para metadados de bases\n",
    "data quality"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5860577388461928,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
