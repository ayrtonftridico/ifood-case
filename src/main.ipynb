{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b6233c-2aed-402e-9726-16d0843f6120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/Volumes/workspace/nyc_taxi/raw/2023/*.parquet\")\n",
    "display(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd251d7-a7e8-4fc5-b753-f480c4803916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "base_dir = \"/Volumes/workspace/nyc_taxi/raw/2023\"\n",
    "\n",
    "# Schema alvo (padronizado)\n",
    "target_schema = {\n",
    "    \"vendorid\": \"long\",\n",
    "    \"tpep_pickup_datetime\": \"timestamp\",\n",
    "    \"tpep_dropoff_datetime\": \"timestamp\",\n",
    "    \"passenger_count\": \"double\",\n",
    "    \"trip_distance\": \"double\",\n",
    "    \"ratecodeid\": \"double\",\n",
    "    \"store_and_fwd_flag\": \"string\",\n",
    "    \"pulocationid\": \"long\",\n",
    "    \"dolocationid\": \"long\",\n",
    "    \"payment_type\": \"long\",\n",
    "    \"fare_amount\": \"double\",\n",
    "    \"extra\": \"double\",\n",
    "    \"mta_tax\": \"double\",\n",
    "    \"tip_amount\": \"double\",\n",
    "    \"tolls_amount\": \"double\",\n",
    "    \"improvement_surcharge\": \"double\",\n",
    "    \"total_amount\": \"double\",\n",
    "    \"congestion_surcharge\": \"double\",\n",
    "    \"airport_fee\": \"double\",\n",
    "}\n",
    "\n",
    "target_cols = list(target_schema.keys())\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(base_dir) if f.path.endswith(\".parquet\")]\n",
    "assert files, f\"Nenhum parquet encontrado em {base_dir}\"\n",
    "\n",
    "def normalize_one(path: str):\n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    # padroniza nomes de colunas\n",
    "    for c in df.columns:\n",
    "        new = c.lower()\n",
    "        if new != c:\n",
    "            df = df.withColumnRenamed(c, new)\n",
    "\n",
    "    if \"airport_fee\" not in df.columns and \"Airport_fee\".lower() in [c.lower() for c in df.columns]:\n",
    "        df = df.withColumnRenamed(\"Airport_fee\", \"airport_fee\")\n",
    "\n",
    "    for col, dtype in target_schema.items():\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None).cast(dtype))\n",
    "\n",
    "    for col, dtype in target_schema.items():\n",
    "        df = df.withColumn(col, F.col(col).cast(dtype))\n",
    "\n",
    "    # usa _metadata.file_path (suportado no Unity Catalog)\n",
    "    df = df.withColumn(\n",
    "        \"anomes\",\n",
    "        F.regexp_replace(\n",
    "            F.regexp_extract(F.col(\"_metadata.file_path\"), r\".*_(\\d{4}-\\d{2})\\.parquet$\", 1),\n",
    "            \"-\", \"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df.select(*target_cols, \"anomes\")\n",
    "\n",
    "dfs = [normalize_one(p) for p in files]\n",
    "df_all = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "df_all.select(\"anomes\").distinct().orderBy(\"anomes\").show()\n",
    "df_all.printSchema()\n",
    "\n",
    "(df_all.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"anomes\")\n",
    "    .saveAsTable(\"workspace.nyc_taxi.yellowtaxi_trips_2023\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0706e77f-51bb-4d41-858b-264882d11e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Leitura da tabela Bronze (que você criou anteriormente)\n",
    "df_bronze = spark.table(\"workspace.nyc_taxi.yellowtaxi_trips_2023\")\n",
    "\n",
    "# Filtro de colunas e de período\n",
    "df_silver = (df_bronze\n",
    "    .select(\"vendorid\",\n",
    "            \"passenger_count\",\n",
    "            \"total_amount\",\n",
    "            \"tpep_pickup_datetime\",\n",
    "            \"tpep_dropoff_datetime\",\n",
    "            \"anomes\")\n",
    "    .filter((F.col(\"anomes\") >= \"202301\") & (F.col(\"anomes\") <= \"202305\"))\n",
    ")\n",
    "\n",
    "# Escrita em uma nova tabela Delta particionada por anomes\n",
    "(df_silver.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"anomes\")\n",
    "    .saveAsTable(\"workspace.nyc_taxi.yellowtaxi_trips_2023_silver\"))\n",
    "\n",
    "display(df_silver.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d553114b-a338-468a-a522-6a6e9b6fea39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT anomes,\n",
    "       COUNT(*) AS qtd_linhas\n",
    "FROM workspace.nyc_taxi.yellowtaxi_trips_2023_silver\n",
    "GROUP BY anomes\n",
    "ORDER BY anomes;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d90d4e-a6e2-4221-abce-381d1862e923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT anomes,\n",
    "       ROUND(AVG(total_amount), 2) AS media_total_amount\n",
    "FROM workspace.nyc_taxi.yellowtaxi_trips_2023_silver\n",
    "GROUP BY anomes\n",
    "ORDER BY anomes;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a428cf1-eeac-486b-9243-7df5ef902d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT HOUR(tpep_pickup_datetime) AS hora_do_dia,\n",
    "       ROUND(AVG(passenger_count), 2) AS media_passageiros\n",
    "FROM workspace.nyc_taxi.yellowtaxi_trips_2023_silver\n",
    "WHERE anomes = '202305'\n",
    "GROUP BY hora_do_dia\n",
    "ORDER BY hora_do_dia;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a1ce56-459c-4720-86c5-59995b6aaa97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Consulta e coleta os dados\n",
    "df_maio = (spark.table(\"workspace.nyc_taxi.yellowtaxi_trips_2023_silver\")\n",
    "    .filter(F.col(\"anomes\") == \"202305\")\n",
    "    .groupBy(F.hour(\"tpep_pickup_datetime\").alias(\"hora_do_dia\"))\n",
    "    .agg(F.round(F.avg(\"passenger_count\"), 2).alias(\"media_passageiros\"))\n",
    "    .orderBy(\"hora_do_dia\")\n",
    ")\n",
    "\n",
    "# Converte para pandas para plotar\n",
    "pdf = df_maio.toPandas()\n",
    "\n",
    "# 2. Cria o gráfico\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pdf[\"hora_do_dia\"], pdf[\"media_passageiros\"], marker=\"o\")\n",
    "plt.title(\"Média de Passageiros por Hora - Maio/2023\")\n",
    "plt.xlabel(\"Hora do Dia\")\n",
    "plt.ylabel(\"Média de Passageiros\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5860577388461930,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
