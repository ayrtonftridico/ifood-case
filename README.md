# üöñ iFood Data Architecture Case ‚Äî NYC Yellow Taxi (README aprimorado)

Este reposit√≥rio implementa um pipeline **reprodut√≠vel** para ingest√£o, padroniza√ß√£o, modelagem e an√°lise dos dados p√∫blicos da **NYC TLC** (Yellow Taxi) com **PySpark** e **Delta Lake**, pensado para rodar no **Databricks Community Edition** (sem acesso √† internet).

> **Escopo do case**: ingerir dados **Jan‚ÄìMai/2023**, disponibilizar via SQL, e responder perguntas anal√≠ticas espec√≠ficas. Tamb√©m s√£o exigidas as colunas: `VendorID`, `passenger_count`, `total_amount`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`.


## üß≠ Sum√°rio
- [Arquitetura de Dados](#arquitetura-de-dados)
- [Padr√µes de Entrega & Conformidade](#padr√µes-de-entrega--conformidade)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Como obter os dados (offline)](#como-obter-os-dados-offline)
- [Execu√ß√£o no Databricks CE (passo a passo)](#execu√ß√£o-no-databricks-ce-passo-a-passo)
- [Modelagem e Particionamento](#modelagem-e-particionamento)
- [Consultas de Resposta (SQL)](#consultas-de-resposta-sql)
- [Valida√ß√µes de Qualidade de Dados](#valida√ß√µes-de-qualidade-de-dados)
- [Dicion√°rio de Dados (Silver)](#dicion√°rio-de-dados-silver)
- [Boas pr√°ticas e performance](#boas-pr√°ticas-e-performance)
- [Erros comuns & Troubleshooting](#erros-comuns--troubleshooting)
- [Estrutura do Reposit√≥rio](#estrutura-do-reposit√≥rio)
- [Licen√ßa](#licen√ßa)


## üèõÔ∏è Arquitetura de Dados

Camadas (Data Lake):
- **RAW (landing)**: arquivos originais exatamente como baixados (Parquet). _Somente leitura_.
- **Bronze (SOR)**: normaliza√ß√£o m√≠nima de tipos/nomes, adi√ß√£o de coluna t√©cnica `anomes` (YYYYMM), controle de ingest√£o.
- **Silver (consumo)**: proje√ß√£o e filtragem para o escopo do case (Jan‚ÄìMai/2023) e **apenas as colunas obrigat√≥rias**.
- **(Opcional) Gold**: agrega√ß√µes e m√©tricas derivadas (n√£o requerido, mas √∫til para apresenta√ß√µes).

Tecnologias:
- **PySpark** para ETL/ELT.
- **Delta Lake** para armazenamento, versionamento e _time travel_.
- **SQL** para consultas anal√≠ticas aos consumidores.


## ‚úÖ Padr√µes de Entrega & Conformidade

Este README incorpora e **mapeia os requisitos do case** √†s entregas do projeto:

- **Ingest√£o no Data Lake** dos dados Yellow Taxi: **OK** (se√ß√µes _Como obter os dados_ e _Execu√ß√£o no Databricks_).  
- **Disponibiliza√ß√£o para consumo (SQL)**: **OK** (tabelas Delta, se√ß√£o _Consultas de Resposta_).  
- **Uso de PySpark**: **OK** (pipelines nas camadas Bronze/Silver).  
- **Colunas obrigat√≥rias presentes**: `VendorID`, `passenger_count`, `total_amount`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`.  
- **Per√≠odo Jan‚ÄìMai/2023**: **OK**, filtrado na Silver.  
- **An√°lises respondidas**: consultas SQL prontas (m√©dia `total_amount` por m√™s e m√©dia `passenger_count` por hora em Maio).  
- **Qualidade, organiza√ß√£o e justificativas**: ver _Arquitetura_, _Valida√ß√µes_ e _Boas pr√°ticas_.


## üß© Pr√©-requisitos

- Conta no **Databricks Community Edition**.
- **Sem internet no cluster**: o download deve ser feito **fora** do Databricks e os arquivos enviados para um **Volume**.
- Python 3.10+ (apenas se for baixar localmente).  
- O arquivo `requirements.txt` lista depend√™ncias para execu√ß√£o local (n√£o necess√°rias no Databricks).


## üì• Como obter os dados (offline)

1) **Baixe localmente** os Parquet de **Yellow Taxi** (Jan‚ÄìMai/2023). Um script de exemplo (fora do Databricks) est√° em `scripts/download_tlc.py` (ou use o snippet deste README).  
2) **Envie os arquivos** via UI do Databricks para um **Volume** (recomendado) ou pasta gerenciada:

```
/Volumes/<catalog>/<schema>/nyc_taxi/raw/2023/
```

> **Por que Volume?** O Databricks CE costuma restringir DBFS p√∫blico e o cluster n√£o tem internet. Volumes funcionam bem com Unity Catalog.


## ‚ñ∂Ô∏è Execu√ß√£o no Databricks CE (passo a passo)

> **Observa√ß√£o**: Se seu ambiente **n√£o** tiver Unity Catalog, substitua nomes `catalog.schema.tabela` por `hive_metastore.default.tabela` e paths de `/Volumes/...` por um caminho suportado (ex.: `/mnt/...`).

1. **Crie cat√°logo/esquema/volumes** (Unity Catalog):
   ```sql
   CREATE CATALOG IF NOT EXISTS workspace;
   CREATE SCHEMA  IF NOT EXISTS workspace.nyc_taxi;
   CREATE VOLUME  IF NOT EXISTS workspace.nyc_taxi.raw;
   CREATE VOLUME  IF NOT EXISTS workspace.nyc_taxi.silver;
   ```

2. **Fa√ßa upload** dos arquivos Parquet (Jan‚ÄìMai/2023) para:
   ```
   /Volumes/workspace/nyc_taxi/raw/2023/
   ```

3. **Bronze ‚Äî ingest√£o e normaliza√ß√£o m√≠nima**  
   Execute o notebook `01_ingestao_bronze.py`, que:
   - L√™ todos os Parquet em `/Volumes/workspace/nyc_taxi/raw/2023/*.parquet`
   - Padroniza schema e nomes
   - Cria `anomes` (YYYYMM) a partir de `tpep_pickup_datetime`
   - Persiste a Delta Table:
     ```
     workspace.nyc_taxi.yellowtaxi_trips_sor
     ```

4. **Silver ‚Äî proje√ß√£o para o case**
   Execute `02_transformacao_silver.py`, que:
   - Seleciona **apenas** as colunas obrigat√≥rias
   - Filtra **2023-01** a **2023-05**
   - Particiona por `anomes` e salva como Delta:
     ```
     workspace.nyc_taxi.yellowtaxi_trips_2023_silver
     ```

5. **An√°lises**
   Execute `03_analises.py` (ou as consultas SQL abaixo).  
   (Opcional) `04_visualizacoes.py` gera gr√°ficos de apoio.


## üß± Modelagem e Particionamento

- **Chave de parti√ß√£o**: `anomes` (YYYYMM) calculado de `tpep_pickup_datetime` para refletir a **data efetiva do evento**.  
- **Extra√ß√£o de metadados do arquivo**: quando necess√°rio, use `_metadata.file_path` (Spark/Delta) ‚Äî substitui `input_file_name()` em ambientes UC.  
- **Nomes padronizados**: `snake_case`, colunas castadas para tipos consistentes.


## üß™ Consultas de Resposta (SQL)

**1) M√©dia de `total_amount` por m√™s (Jan‚ÄìMai/2023):**
```sql
SELECT anomes, ROUND(AVG(total_amount), 2) AS media_total_amount
FROM workspace.nyc_taxi.yellowtaxi_trips_2023_silver
GROUP BY anomes
ORDER BY anomes;
```

**2) M√©dia de `passenger_count` por hora do dia em Maio/2023:**
```sql
SELECT HOUR(tpep_pickup_datetime) AS hora_do_dia,
       ROUND(AVG(passenger_count), 2) AS media_passageiros
FROM workspace.nyc_taxi.yellowtaxi_trips_2023_silver
WHERE anomes = '202305'
GROUP BY hora_do_dia
ORDER BY hora_do_dia;
```


## üîç Valida√ß√µes de Qualidade de Dados

- **Presen√ßa de colunas obrigat√≥rias** (fail-fast): checar exist√™ncia no DataFrame antes de escrever a Silver.
- **Controles de nulidade**: `VendorID`, `tpep_pickup_datetime`, `tpep_dropoff_datetime` n√£o devem ser nulos na Silver.
- **Regras de dom√≠nio** (exemplos):
  - `passenger_count >= 0`
  - `total_amount` pode ser negativo (ajustes), mas monitore outliers extremos.
  - `tpep_dropoff_datetime >= tpep_pickup_datetime`
- **Volumetria por parti√ß√£o**: alerta se parti√ß√µes de `anomes` tiverem volume an√¥malo.


## üìö Dicion√°rio de Dados (Silver)

| Campo                   | Tipo      | Descri√ß√£o                                               |
|------------------------|-----------|---------------------------------------------------------|
| `vendorid`             | INT       | Identificador do provedor                               |
| `passenger_count`      | INT       | N√∫mero de passageiros                                   |
| `total_amount`         | DOUBLE    | Valor total da corrida                                  |
| `tpep_pickup_datetime` | TIMESTAMP | Data/hora do embarque                                   |
| `tpep_dropoff_datetime`| TIMESTAMP | Data/hora do desembarque                                |
| `anomes`               | STRING    | Parti√ß√£o no formato `YYYYMM` (derivada do pickup time)  |


## ‚öôÔ∏è Boas pr√°ticas e performance

- **Delta Lake**: transa√ß√µes ACID e _time travel_ para auditoria e reprocessos.
- **Particionamento por `anomes`**: melhora _pruning_ em filtros mensais.
- **Auto-Optimize** (se dispon√≠vel) e **OPTIMIZE + ZORDER** (opcional) em colunas de tempo.
- **Idempot√™ncia**: pipelines projetados para reprocesso seguro de parti√ß√µes.


## üßØ Erros comuns & Troubleshooting

- **Sem internet no Databricks CE / `UnknownHostException`**: baixe os Parquet **fora** do cluster e fa√ßa upload para **Volumes**.
- **`Public DBFS root is disabled`**: prefira `/Volumes/<catalog>/<schema>/<volume>/...`.
- **Unity Catalog ‚Äî `input_file_name()` n√£o suportado**: use **`_metadata.file_path`** para extrair ano/m√™s do caminho.
- **`DATATYPE_MISMATCH` ao concatenar strings** no Spark SQL: utilize `concat()` ou `format_string()` em vez de `+` para strings.


## üóÇÔ∏è Estrutura do Reposit√≥rio

```
ifood-case/
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ 01_ingestao_bronze.py
‚îÇ  ‚îú‚îÄ 02_transformacao_silver.py
‚îÇ  ‚îú‚îÄ 03_analises.py
‚îÇ  ‚îî‚îÄ 04_visualizacoes.py              # opcional
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ download_tlc.py                  # download offline (local)
‚îú‚îÄ notebooks/                          # (opcional) vers√µes em notebook
‚îú‚îÄ README.md
‚îî‚îÄ requirements.txt
```


## üìÑ Licen√ßa

Uso educacional para execu√ß√£o do case t√©cnico. Dados originais pertencem √† NYC TLC (uso p√∫blico).


---

### üìé Snippet opcional ‚Äî Download local (fora do Databricks)

```python
import os, requests
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_URL = "https://d37ci6vzurychx.cloudfront.net/trip-data/{}_tripdata_{}-{}.parquet"
CATEGORIES = ["yellow"]
YEARS = ["2023"]
MONTHS = [f"{i:02d}" for i in range(1, 6)]  # Jan‚ÄìMai

OUTPUT_DIR = "nyc_taxi_raw"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def download_file(cat, year, month):
    url = BASE_URL.format(cat, year, month)
    filename = os.path.join(OUTPUT_DIR, f"{cat}_{year}-{month}.parquet")
    if os.path.exists(filename):
        return f"‚è© J√° existe: {filename}"
    try:
        r = requests.get(url, timeout=60)
        r.raise_for_status()
        with open(filename, "wb") as f:
            f.write(r.content)
        return f"‚úÖ Baixado: {url}"
    except Exception as e:
        return f"‚ö†Ô∏è Erro ao baixar {url}: {e}"

with ThreadPoolExecutor(max_workers=8) as ex:
    futures = [ex.submit(download_file, c, y, m) for c in CATEGORIES for y in YEARS for m in MONTHS]
    for f in as_completed(futures):
        print(f.result())
```
