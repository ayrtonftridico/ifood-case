# üöñ iFood Data Architecture Case ‚Äî NYC Taxis (Yellow, Green, FHV, FHVHV)

Este projeto implementa um pipeline **reprodut√≠vel** para ingest√£o, padroniza√ß√£o, modelagem e an√°lise dos dados p√∫blicos da **NYC TLC** usando **PySpark** e **Delta Lake** no **Databricks Community Edition** (sem acesso √† internet).

> - **Yellow**: Foram utilizados todos os arquivos de **2020‚Äì2025** para exemplificar pr√°ticas de camadas bronze-silver (robustez e volume).
> - **Green, FHV & FHVHV:** apenas **maio/2023** (suficiente para a segunda quest√£o).
---

## üß≠ Sum√°rio
- [Arquitetura de Dados](#%EF%B8%8F-arquitetura-de-dados)
- [Cobertura de Dados (por frota)](#-cobertura-de-dados-por-frota)
- [Pr√©-requisitos](#-pr√©-requisitos)
- [Como obter os dados (offline)](#-como-obter-os-dados-offline)
- [Execu√ß√£o no Databricks CE (passo a passo)](#%EF%B8%8F-execu√ß√£o-no-databricks-ce-passo-a-passo)
- [Consultas de Resposta (SQL)](#-consultas-de-resposta-sql)
- [Valida√ß√µes de Qualidade de Dados](#-valida√ß√µes-de-qualidade-de-dados)
- [Boas pr√°ticas e performance](#%EF%B8%8F-boas-pr√°ticas-e-performance)
- [Estrutura do Reposit√≥rio](#%EF%B8%8F-estrutura-do-reposit√≥rio)

---

## üèõÔ∏è Arquitetura de Dados

Camadas (Data Lake):
- **RAW**: arquivos originais exatamente como baixados (Parquet).
- **Bronze**: normaliza√ß√£o m√≠nima de tipos/nomes, coluna t√©cnica `anomes` (YYYYMM) derivada da data de pickup de cada schema.
- **Silver**: aplica√ß√£o de conceitos de Data Quality. Remove linhas com colunas essenciais vazias, linhas com pickupdate < dropoffdate, linhas com passageiro =< 0.
- **Gold**: cria√ß√£o das duas camadas gold utilizadas especificamente para a resposta das quest√µes do case. A primeira √© uma tabela com as informa√ß√µes das viagens do yellow taxi de janeiro a maio de 2023. A segunda √© uma jun√ß√£o das informa√ß√µes de pickupdate e n√∫mero de passageiros(apenas yellow e green possuem) de todos os tipos de taxi para maio de 2023.

Tecnologias:
- **PySpark** para ETL/ELT, **Delta Lake** para armazenamento, **SQL** para consumo anal√≠tico.

---

## üìä Cobertura de Dados (por frota)

| Frota   | Per√≠odo carregado |
|---------|--------------------|
| Yellow  | 2020‚Äì2025 (todos os meses dispon√≠veis) | 
| Green   | **somente 2023-05**                     | 
| FHV     | **somente 2023-05**                     | 
| FHVHV   | **somente 2023-05**                     | 

> **Nota metodol√≥gica:** quando uma m√©trica depende de uma coluna ausente numa frota (p.ex. `passenger_count`), a agrega√ß√£o considera **apenas as frotas que fornecem o campo** (Yellow/Green). Isso √© explicitado nas consultas SQL.

---

## üß© Pr√©-requisitos

- Conta no **Databricks Community Edition** (cluster sem internet).
- Download **offline** pr√©vio (feito localmente) e upload para os **Volumes**
- Python 3.10+ localmente apenas se for usar o script de download.
- O `requirements.txt` auxilia na execu√ß√£o local (n√£o √© necess√°rio no Databricks).

---

## üì• Como obter os dados (offline)

Como o Databricks CE **n√£o tem internet**, baixe localmente e depois fa√ßa **upload** para um **Volume**. Script exemplo (ajuste anos/meses conforme sua estrat√©gia):

```python
import os
import time
import requests

BASE = "https://d37ci6vzurychx.cloudfront.net/trip-data/{}_tripdata_{}-{}.parquet"

# Ajuste aqui os anos que deseja por categoria:
YEARS_BY_CAT = {
    "yellow": [str(y) for y in range(2020, 2026)],  # 2020‚Äì2025 (todos os meses)
    "green":  ["2023"],                              # mensal (mude se quiser mais anos)
    "fhv":    ["2023"],                              # mensal
    "fhvhv":  ["2023"],                              # mensal
}

MONTHS = [f"{m:02d}" for m in range(1, 13)]          # meses 01..12
OUT_DIR = "nyc_taxi_raw"                             # pasta de sa√≠da local
TIMEOUT = 60                                         # segundos
SLEEP_BETWEEN = 0.25                                 # pausa entre downloads (evita throttling)

os.makedirs(OUT_DIR, exist_ok=True)

def download_one(category: str, year: str, month: str) -> None:
    """
    Baixa um √∫nico arquivo mensal para a categoria/ano/m√™s informados.
    Usa streaming (chunks) para n√£o carregar tudo em mem√≥ria.
    Pula se o arquivo j√° existir.
    """
    url = BASE.format(category, year, month)
    out_path = os.path.join(OUT_DIR, f"{category}_{year}-{month}.parquet")

    if os.path.exists(out_path):
        print(f"[skip] {out_path}")
        return

    try:
        with requests.get(url, timeout=TIMEOUT, stream=True) as r:
            if r.status_code != 200:
                print(f"[miss] {url} (HTTP {r.status_code})")
                return

            with open(out_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=1024 * 1024):  # 1MB
                    if chunk:
                        f.write(chunk)

        print(f"[ ok ] {url}")
    except Exception as e:
        print(f"[err ] {url} -> {e}")

def main():
    total_planned = 0
    for cat, years in YEARS_BY_CAT.items():
        for y in years:
            for m in MONTHS:
                total_planned += 1

    print(f"üîé Planejados {total_planned} arquivos para download em '{OUT_DIR}'.")

    downloaded = 0
    for cat, years in YEARS_BY_CAT.items():
        for y in years:
            for m in MONTHS:
                download_one(cat, y, m)
                downloaded += 1
                time.sleep(SLEEP_BETWEEN)

    print(f"‚úÖ Processo conclu√≠do. Arquivos tentados: {downloaded}.")

if __name__ == "__main__":
    main()

```

**Upload sugerido (Volumes):**
```
/Volumes/workspace/nyc_taxi/raw/yellow/*.parquet
/Volumes/workspace/nyc_taxi/raw/green/*.parquet
/Volumes/workspace/nyc_taxi/raw/fhv/*.parquet
/Volumes/workspace/nyc_taxi/raw/fhvhv/*.parquet
```

---

## ‚ñ∂Ô∏è Execu√ß√£o no Databricks CE (passo a passo)

1. **Crie os Volumes e fa√ßa o upload** conforme a estrutura indicada (Parquets por frota/pasta).

2. **Bronze ‚Äî ingest√£o e normaliza√ß√£o m√≠nima**  
   Notebook `01_ingestao_bronze.ipynb`  
   - Leitura **por categoria** (yellow/green/fhv/fhvhv) a partir dos diret√≥rios em `/Volumes/...`.  
   - **Padroniza√ß√£o de nomes** (lower/sanitiza√ß√£o) e **casts** para o **schema-alvo da frota**.  
   - Deriva√ß√£o de **`anomes` (YYYYMM)** a partir da **coluna de pickup informada** para cada frota.  
   - Escrita em **Delta** particionada por `anomes`:
     ```
     workspace.nyc_taxi.yellow_trips_bronze
     workspace.nyc_taxi.green_trips_bronze
     workspace.nyc_taxi.fhv_trips_bronze
     workspace.nyc_taxi.fhvhv_trips_bronze
     ```

3. **Silver ‚Äî limpeza simples**  
   Notebook `02_transformacao_silver.ipynb`  
   Para **cada** tabela Bronze, aplicar as mesmas regras gen√©ricas:
   - **Remover linhas** com **colunas essenciais nulas** (ex.: `anomes`, pickup/dropoff; `vendorid` quando existir).  
   - **Remover parti√ß√µes** cujo volume seja **< 3%** da parti√ß√£o mais populosa (da **pr√≥pria** tabela).  
   - **Remover registros** com **ordem temporal inv√°lida** (`dropoff < pickup`).  
   - **Remover linhas** com `passenger_count ‚â§ 0` **se** a coluna existir na frota.  
   - **Logar** a quantidade de linhas removidas **por regra**.  
   - Gravar **Delta** particionada por `anomes`:
     ```
     workspace.nyc_taxi.yellow_trips_silver
     workspace.nyc_taxi.green_trips_silver
     workspace.nyc_taxi.fhv_trips_silver
     workspace.nyc_taxi.fhvhv_trips_silver
     ```

4. **Gold ‚Äî modelagem anal√≠tica (√∫ltimo ajuste)**  
   Notebook `03_transformacao_gold.ipynb`  
   - **Gold 1 ‚Äì Yellow (2023/01‚Äì2023/05)**  
     Seleciona **apenas** as colunas do abaixo e filtra o per√≠odo:  
     `vendorid, passenger_count, total_amount, tpep_pickup_datetime, tpep_dropoff_datetime, anomes`  
     Tabela (Delta, particionada por `anomes`):  
     ```
     workspace.nyc_taxi.yellow_2023_jan_may_gold
     ```
   - **Gold 2 ‚Äì Maio/2023 unificada (m√≠nimo necess√°rio)**  
     Consolida **todas as frotas** apenas para `anomes = '202305'` para calculo da m√©dia de passageiros por hora:  
     `pickup_datetime, passenger_count, anomes`  
     > `passenger_count` **n√£o existe** em FHV/FHVHV ‚Äî fica **NULL** e n√£o afeta o `AVG`.  
     Tabela (Delta, particionada por `anomes`):  
     ```
     workspace.nyc_taxi.may_2023_min_gold
     ```

---

## üß™ Consultas de Resposta (SQL)

**1) M√©dia de `total_amount` por m√™s (Jan‚ÄìMai/2023, apenas frotas que possuem `total_amount`):**
```sql
SELECT anomes,
       ROUND(AVG(total_amount), 2) AS media_total_amount
FROM workspace.nyc_taxi.trips_2023_silver
WHERE anomes BETWEEN '202301' AND '202305'
  AND total_amount IS NOT NULL
GROUP BY anomes
ORDER BY anomes;
```

**2) M√©dia de `passenger_count` por hora do dia em Maio/2023 (frotas que possuem `passenger_count`: Yellow/Green):**
```sql
SELECT HOUR(pickup_datetime) AS hora_do_dia,
       ROUND(AVG(passenger_count), 2) AS media_passageiros
FROM workspace.nyc_taxi.trips_2023_silver
WHERE anomes = '202305'
  AND passenger_count IS NOT NULL
GROUP BY hora_do_dia
ORDER BY hora_do_dia;
```

---

## üîç Valida√ß√µes de Qualidade de Dados

- **Presen√ßa de colunas obrigat√≥rias** na Silver.
- **Nulidade e dom√≠nios:**
  - `pickup_datetime`/`dropoff_datetime` n√£o nulos.
  - `passenger_count >= 0` quando existir.
  - `total_amount >= 0` (admitindo eventuais negativos por ajustes, mas monitorando outliers).
  - `dropoff_datetime >= pickup_datetime`.
- **Volumetria por parti√ß√£o** e alerta para `anomes` com baix√≠ssimo volume.

---

## ‚öôÔ∏è Boas pr√°ticas e performance

- **Delta Lake**.
- **Leitura em lote** por diret√≥rio e `unionByName` com `allowMissingColumns` para esquemas diferentes.
- **`repartition("anomes")`** antes de escrever, melhorando o layout f√≠sico das parti√ß√µes.

---

## üóÇÔ∏è Estrutura do Reposit√≥rio

```
ifood-case/
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ 01_ingestao_bronze.ipynb
‚îÇ  ‚îú‚îÄ 02_transformacao_silver.ipynb
‚îÇ  ‚îî‚îÄ 03_transformacao_gold.ipynb
‚îú‚îÄ analysis/
‚îÇ  ‚îî‚îÄ analysis.ipynb
‚îú‚îÄ README.md
‚îî‚îÄ requirements.txt
```

---

## üìå Notas finais

- As escolhas de cobertura por frota (**2020‚Äì2025** para Yellow; **maio/2023** para Green/FHV/FHVHV) foram feitas para **equilibrar escala** e **limita√ß√µes** da vers√£o teste do Databricks
