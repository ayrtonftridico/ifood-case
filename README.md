# üöñ iFood Data Architecture Case ‚Äî NYC Yellow Taxi

Este reposit√≥rio implementa um pipeline **reprodut√≠vel** para ingest√£o, padroniza√ß√£o, modelagem e an√°lise dos dados p√∫blicos da **NYC TLC** (Yellow Taxi) com **PySpark** e **Delta Lake**, pensado para rodar no **Databricks Community Edition** (sem acesso √† internet).

> **Escopo do case**: ingerir dados **Jan‚ÄìMai/2023**, disponibilizar via SQL, e responder perguntas anal√≠ticas espec√≠ficas. Tamb√©m s√£o exigidas as colunas: `VendorID`, `passenger_count`, `total_amount`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`.


## üß≠ Sum√°rio
- [Arquitetura de Dados](#arquitetura-de-dados)
- [Padr√µes de Entrega & Conformidade](#padr√µes-de-entrega--conformidade)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Como obter os dados (offline)](#como-obter-os-dados-offline)
- [Execu√ß√£o no Databricks CE (passo a passo)](#execu√ß√£o-no-databricks-ce-passo-a-passo)
- [Modelagem e Particionamento](#modelagem-e-particionamento)
- [Consultas de Resposta (SQL)](#consultas-de-resposta-sql)
- [Valida√ß√µes de Qualidade de Dados](#valida√ß√µes-de-qualidade-de-dados)
- [Dicion√°rio de Dados (Silver)](#dicion√°rio-de-dados-silver)
- [Boas pr√°ticas e performance](#boas-pr√°ticas-e-performance)
- [Estrutura do Reposit√≥rio](#estrutura-do-reposit√≥rio)

## üèõÔ∏è Arquitetura de Dados

Camadas (Data Lake):
- **RAW (landing)**: arquivos originais exatamente como baixados (Parquet). _Somente leitura_.
- **Bronze (SOR)**: base com historico desde 2020 com normaliza√ß√£o m√≠nima de tipos/nomes, adi√ß√£o de coluna t√©cnica `anomes` (YYYYMM).
- **Silver (consumo)**: proje√ß√£o e filtragem para o escopo do case (Jan‚ÄìMai/2023) e **apenas as colunas obrigat√≥rias** e um leve filtro de data quality.

Tecnologias:
- **PySpark** para ETL/ELT.
- **Delta Lake** para armazenamento e versionamento.
- **SQL** para consultas anal√≠ticas aos consumidores.


## ‚úÖ Padr√µes de Entrega & Conformidade

Este README incorpora e **mapeia os requisitos do case** √†s entregas do projeto:

- **Ingest√£o no Data Lake** dos dados Yellow Taxi: **OK** (se√ß√µes _Como obter os dados_ e _Execu√ß√£o no Databricks_).  
- **Disponibiliza√ß√£o para consumo (SQL)**: **OK** (tabelas Delta, se√ß√£o _Consultas de Resposta_).  
- **Uso de PySpark**: **OK** (pipelines nas camadas Bronze/Silver).  
- **Colunas obrigat√≥rias presentes**: `VendorID`, `passenger_count`, `total_amount`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`.  
- **Per√≠odo Jan‚ÄìMai/2023**: **OK**, filtrado na Silver.  
- **An√°lises respondidas**: consultas SQL prontas (m√©dia `total_amount` por m√™s e m√©dia `passenger_count` por hora em Maio).  
- **Qualidade, organiza√ß√£o e justificativas**: ver _Arquitetura_, _Valida√ß√µes_ e _Boas pr√°ticas_.


## üß© Pr√©-requisitos

- Conta no **Databricks Community Edition**.
- **Sem internet no cluster**: o download deve ser feito **fora** do Databricks e os arquivos enviados para um **Volume**.
- Python 3.10+ (apenas se for baixar localmente).  
- O arquivo `requirements.txt` lista depend√™ncias para execu√ß√£o local (n√£o necess√°rias no Databricks).


## üì• Como obter os dados (offline)

1) **Baixe localmente** os Parquet de **Yellow Taxi** (Jan‚ÄìMai/2023).

Script exemplo abaixo:

```python
import os, requests

OUT = "nyc_taxi_yellow_2020_2025"
os.makedirs(OUT, exist_ok=True)

for year in range(2020, 2026):
    for month in range(1, 13):
        url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet"
        path = os.path.join(OUT, f"yellow_{year}-{month:02d}.parquet")

        if os.path.exists(path):
            print("skip", path)
            continue

        r = requests.get(url)
        if r.status_code == 200:
            with open(path, "wb") as f:
                f.write(r.content)
            print("ok", url)
        else:
            print("miss", url, r.status_code)

```
2) **Envie os arquivos** via UI do Databricks para um **Volume** (recomendado) ou pasta gerenciada:

```
/Volumes/<catalog>/<schema>/nyc_taxi/raw/2023/
```


## ‚ñ∂Ô∏è Execu√ß√£o no Databricks CE (passo a passo)

1. **Crie o Volume e Fa√ßa upload** dos arquivos Parquet (Jan‚ÄìMai/2023) para:
   ```
   /Volumes/workspace/nyc_taxi/raw/2023/
   ```

2. **Bronze ‚Äî ingest√£o e normaliza√ß√£o m√≠nima**  
   Execute o notebook `01_ingestao_bronze.ipynb`, que:
   - L√™ todos os Parquet em `/Volumes/workspace/nyc_taxi/raw/2023/*.parquet`
   - Padroniza schema e nomes
   - Cria `anomes` (YYYYMM) a partir de `tpep_pickup_datetime`
   - Persiste a Delta Table:
     ```
     workspace.nyc_taxi.yellowtaxi_trips_sor
     ```

3. **Silver ‚Äî proje√ß√£o para o case**
   Execute `02_transformacao_silver.ipynb`, que:
   - Seleciona **apenas** as colunas obrigat√≥rias
   - Filtra **2023-01** a **2023-05**
   - Particiona por `anomes` e salva como Delta:
     ```
     workspace.nyc_taxi.yellowtaxi_trips_2023_spec
     ```

4. **An√°lises**
   Execute `analysis.ipynb` (ou as consultas SQL abaixo).  


## üß± Modelagem e Particionamento

- **Chave de parti√ß√£o**: `anomes` (YYYYMM) calculado de `tpep_pickup_datetime` para refletir a **data efetiva do evento**.  
- **Extra√ß√£o de metadados do arquivo**: quando necess√°rio, use `_metadata.file_path` (Spark/Delta) ‚Äî substitui `input_file_name()` em ambientes UC.  
- **Nomes padronizados**: `snake_case`, colunas castadas para tipos consistentes.


## üß™ Consultas de Resposta (SQL)

**1) M√©dia de `total_amount` por m√™s (Jan‚ÄìMai/2023):**
```sql
SELECT anomes, ROUND(AVG(total_amount), 2) AS media_total_amount
FROM workspace.nyc_taxi.yellowtaxi_trips_2023_spec
GROUP BY anomes
ORDER BY anomes;
```

**2) M√©dia de `passenger_count` por hora do dia em Maio/2023:**
```sql
SELECT HOUR(tpep_pickup_datetime) AS hora_do_dia,
       ROUND(AVG(passenger_count), 2) AS media_passageiros
FROM workspace.nyc_taxi.yellowtaxi_trips_2023_spec
WHERE anomes = '202305'
GROUP BY hora_do_dia
ORDER BY hora_do_dia;
```


## üîç Valida√ß√µes de Qualidade de Dados

- **Presen√ßa de colunas obrigat√≥rias** (fail-fast): checar exist√™ncia no DataFrame antes de escrever a Silver.
- **Controles de nulidade**: `VendorID`, `tpep_pickup_datetime`, `tpep_dropoff_datetime` n√£o devem ser nulos na Silver.
- **Regras de dom√≠nio** (exemplos):
  - `passenger_count >= 0`
  - `total_amount` pode ser negativo (ajustes), mas monitore outliers extremos.
  - `tpep_dropoff_datetime >= tpep_pickup_datetime`
- **Volumetria por parti√ß√£o**: alerta se parti√ß√µes de `anomes` tiverem volume an√¥malo.


## üìö Dicion√°rio de Dados (Silver)

| Campo                   | Tipo      | Descri√ß√£o                                               |
|------------------------|-----------|---------------------------------------------------------|
| `vendorid`             | LONG      | Identificador do provedor                               |
| `passenger_count`      | INT       | N√∫mero de passageiros                                   |
| `total_amount`         | DOUBLE    | Valor total da corrida                                  |
| `tpep_pickup_datetime` | TIMESTAMP | Data/hora do embarque                                   |
| `tpep_dropoff_datetime`| TIMESTAMP | Data/hora do desembarque                                |
| `anomes`               | STRING    | Parti√ß√£o no formato `YYYYMM` (derivada do pickup time)  |


## ‚öôÔ∏è Boas pr√°ticas e performance

- **Delta Lake**: transa√ß√µes ACID e _time travel_ para auditoria e reprocessos.
- **Particionamento por `anomes`**: melhora _pruning_ em filtros mensais.

## üóÇÔ∏è Estrutura do Reposit√≥rio

```
ifood-case/
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ 01_ingestao_bronze.ipynb
‚îÇ  ‚îú‚îÄ 02_transformacao_silver.ipynb
‚îú‚îÄ analysis/
‚îÇ  ‚îî‚îÄ analysis.ipynb
‚îú‚îÄ README.md
‚îî‚îÄ requirements.txt
```



